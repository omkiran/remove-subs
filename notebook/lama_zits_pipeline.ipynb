{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7565f692",
   "metadata": {},
   "source": [
    "# ğŸ”§ Unified LaMa + ZITS Video Subtitle Inpainting Pipeline\n",
    "This notebook demonstrates a unified pipeline for removing burnt-in subtitles using LaMa (static frame inpainting) and ZITS (video-aware inpainting).\n",
    "\n",
    "âœ¨ **Features:**\n",
    "- ğŸš€ Auto GPU/CPU detection\n",
    "- ğŸ“¦ Docker containerization\n",
    "- ğŸ”„ Flexible local/cloud deployment\n",
    "- ğŸ¯ Production-ready pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cc6f9",
   "metadata": {},
   "source": [
    "## ğŸ” Step 0: Environment Detection\n",
    "First, let's detect the available hardware and configure the environment accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ee2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to import AWS libraries\n",
    "try:\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError, NoCredentialsError\n",
    "    aws_available = True\n",
    "except ImportError:\n",
    "    aws_available = False\n",
    "\n",
    "# Environment detection\n",
    "def detect_environment():\n",
    "    \"\"\"Detect and configure the runtime environment.\"\"\"\n",
    "    config = {\n",
    "        'gpu_available': torch.cuda.is_available(),\n",
    "        'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'in_docker': os.path.exists('/.dockerenv'),\n",
    "        'in_colab': 'COLAB_GPU' in os.environ,\n",
    "        'aws_available': aws_available,\n",
    "        'aws_configured': False\n",
    "    }\n",
    "    \n",
    "    if config['gpu_available']:\n",
    "        config['gpu_name'] = torch.cuda.get_device_name(0)\n",
    "        config['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    # Check AWS configuration\n",
    "    if aws_available:\n",
    "        try:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.list_buckets()\n",
    "            config['aws_configured'] = True\n",
    "        except (NoCredentialsError, ClientError):\n",
    "            config['aws_configured'] = False\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Detect environment\n",
    "env_config = detect_environment()\n",
    "\n",
    "print(\"ğŸ” Environment Detection:\")\n",
    "print(f\"   GPU Available: {env_config['gpu_available']}\")\n",
    "print(f\"   Device: {env_config['device']}\")\n",
    "print(f\"   Docker: {env_config['in_docker']}\")\n",
    "print(f\"   Colab: {env_config['in_colab']}\")\n",
    "print(f\"   AWS Libraries: {env_config['aws_available']}\")\n",
    "print(f\"   AWS Configured: {env_config['aws_configured']}\")\n",
    "\n",
    "if env_config['gpu_available']:\n",
    "    print(f\"   GPU: {env_config['gpu_name']} ({env_config['gpu_memory']:.1f} GB)\")\n",
    "    print(\"   ğŸš€ GPU acceleration enabled\")\n",
    "else:\n",
    "    print(\"   ğŸ’» Running in CPU mode\")\n",
    "\n",
    "if env_config['aws_configured']:\n",
    "    print(\"   â˜ï¸  AWS S3 access configured\")\n",
    "elif env_config['aws_available']:\n",
    "    print(\"   âš ï¸  AWS libraries available but not configured\")\n",
    "else:\n",
    "    print(\"   ğŸ“¦ Install boto3 for AWS/S3 support\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc921906",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 1: Setup and Data Generation\n",
    "Configure directories and generate synthetic test data with subtitles and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24072f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'input_dir': '../input_video',\n",
    "    'mask_dir': '../masks',\n",
    "    'lama_output_dir': '../lama_output',\n",
    "    'zits_output_dir': '../zits_output',\n",
    "    'frame_count': 30,  # More frames for better testing\n",
    "    'frame_size': (256, 256),\n",
    "    'subtitle_region': (0, 205, 256, 256),  # Bottom portion for subtitles\n",
    "    # AWS/S3 Configuration\n",
    "    's3_input_video': os.environ.get('S3_INPUT_VIDEO', ''),\n",
    "    's3_output_location': os.environ.get('S3_OUTPUT_LOCATION', ''),\n",
    "    'use_synthetic_data': os.environ.get('USE_SYNTHETIC_DATA', 'true').lower() == 'true'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [CONFIG['input_dir'], CONFIG['mask_dir'], CONFIG['lama_output_dir'], CONFIG['zits_output_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"ğŸ“ Created directory: {dir_path}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Pipeline Configuration:\")\n",
    "print(f\"   S3 Input: {CONFIG['s3_input_video'] or 'Not specified'}\")\n",
    "print(f\"   S3 Output: {CONFIG['s3_output_location'] or 'Not specified'}\")\n",
    "print(f\"   Use Synthetic: {CONFIG['use_synthetic_data']}\")\n",
    "\n",
    "# Handle input data based on configuration\n",
    "if CONFIG['s3_input_video'] and env_config['aws_configured']:\n",
    "    print(f\"\\nğŸ“¥ Processing S3 input video: {CONFIG['s3_input_video']}\")\n",
    "    \n",
    "    if aws_available:\n",
    "        try:\n",
    "            # Import S3 handler functionality\n",
    "            s3_client = boto3.client('s3')\n",
    "            \n",
    "            # Parse S3 URL\n",
    "            s3_url = CONFIG['s3_input_video']\n",
    "            if not s3_url.startswith('s3://'):\n",
    "                raise ValueError(\"S3 URL must start with 's3://'\")\n",
    "            \n",
    "            path = s3_url[5:]  # Remove 's3://'\n",
    "            parts = path.split('/', 1)\n",
    "            bucket = parts[0]\n",
    "            key = parts[1]\n",
    "            \n",
    "            # Download video\n",
    "            temp_video = \"/tmp/input_video.mp4\"\n",
    "            print(f\"   Downloading from S3...\")\n",
    "            s3_client.download_file(bucket, key, temp_video)\n",
    "            \n",
    "            # Extract frames using OpenCV\n",
    "            cap = cv2.VideoCapture(temp_video)\n",
    "            frame_count = 0\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Resize frame\n",
    "                frame_resized = cv2.resize(frame, CONFIG['frame_size'])\n",
    "                \n",
    "                # Save frame\n",
    "                frame_path = f\"{CONFIG['input_dir']}/frame_{frame_count+1:05d}.png\"\n",
    "                cv2.imwrite(frame_path, frame_resized)\n",
    "                frame_count += 1\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # Generate masks for all frames (assume bottom subtitle region)\n",
    "            for i in range(frame_count):\n",
    "                mask = np.zeros(CONFIG['frame_size'], dtype=np.uint8)\n",
    "                x1, y1, x2, y2 = CONFIG['subtitle_region']\n",
    "                cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
    "                cv2.imwrite(f\"{CONFIG['mask_dir']}/mask_{i+1:05d}.png\", mask)\n",
    "            \n",
    "            print(f\"âœ… Processed S3 video: {frame_count} frames extracted\")\n",
    "            CONFIG['frame_count'] = frame_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ S3 processing failed: {str(e)}\")\n",
    "            print(\"   Falling back to synthetic data generation...\")\n",
    "            CONFIG['use_synthetic_data'] = True\n",
    "    else:\n",
    "        print(\"âŒ AWS libraries not available\")\n",
    "        CONFIG['use_synthetic_data'] = True\n",
    "\n",
    "elif CONFIG['use_synthetic_data']:\n",
    "    print(f\"\\nğŸ¬ Generating {CONFIG['frame_count']} synthetic test frames...\")\n",
    "\n",
    "    # Generate synthetic frames with subtitles\n",
    "    for i in range(CONFIG['frame_count']):\n",
    "        # Create frame with animated background\n",
    "        img = np.full((*CONFIG['frame_size'], 3), 255, dtype=np.uint8)\n",
    "        \n",
    "        # Add animated gradient effect\n",
    "        phase = i / CONFIG['frame_count'] * 2 * np.pi\n",
    "        for y in range(CONFIG['frame_size'][0]):\n",
    "            for x in range(CONFIG['frame_size'][1]):\n",
    "                # Create moving gradient\n",
    "                r = int(128 + 127 * np.sin(phase + x/30))\n",
    "                g = int(128 + 127 * np.sin(phase + y/30 + np.pi/3))\n",
    "                b = int(128 + 127 * np.sin(phase + (x+y)/40 + 2*np.pi/3))\n",
    "                img[y, x] = [min(255, max(0, r)), min(255, max(0, g)), min(255, max(0, b))]\n",
    "        \n",
    "        # Add main content text\n",
    "        cv2.putText(img, f\"Test Video Frame {i+1:02d}\", (20, 50), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        cv2.putText(img, f\"Sample Content\", (20, 80), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "        \n",
    "        # Add moving object for realistic content\n",
    "        center_x = int(128 + 50 * np.sin(i/5))\n",
    "        center_y = int(120 + 30 * np.cos(i/3))\n",
    "        cv2.circle(img, (center_x, center_y), 15, (0, 255, 255), -1)\n",
    "        \n",
    "        # Add subtitle (to be removed) with variation\n",
    "        subtitle_texts = [\n",
    "            \"This is a sample subtitle\",\n",
    "            \"Subtitle text to remove\", \n",
    "            \"Burnt-in subtitle example\",\n",
    "            f\"Frame {i+1} subtitle\",\n",
    "            \"AI will remove this text\"\n",
    "        ]\n",
    "        subtitle = subtitle_texts[i % len(subtitle_texts)]\n",
    "        \n",
    "        # Add subtitle background box\n",
    "        text_size = cv2.getTextSize(subtitle, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "        cv2.rectangle(img, (10, 215), (text_size[0] + 20, 245), (0, 0, 0), -1)\n",
    "        \n",
    "        # Add subtitle text\n",
    "        cv2.putText(img, subtitle, (15, 235), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        \n",
    "        # Save frame\n",
    "        cv2.imwrite(f\"{CONFIG['input_dir']}/frame_{i+1:05d}.png\", img)\n",
    "\n",
    "        # Create corresponding mask for subtitle area\n",
    "        mask = np.zeros(CONFIG['frame_size'], dtype=np.uint8)\n",
    "        x1, y1, x2, y2 = CONFIG['subtitle_region']\n",
    "        cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
    "        \n",
    "        # Save mask\n",
    "        cv2.imwrite(f\"{CONFIG['mask_dir']}/mask_{i+1:05d}.png\", mask)\n",
    "\n",
    "    print(f\"âœ… Generated {CONFIG['frame_count']} synthetic frames and masks\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  No input specified and synthetic data disabled\")\n",
    "    print(\"   Looking for existing data in directories...\")\n",
    "\n",
    "print(f\"   ğŸ“ Frames: {CONFIG['input_dir']}\")\n",
    "print(f\"   ğŸ­ Masks: {CONFIG['mask_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18559735",
   "metadata": {},
   "source": [
    "## ğŸ‘ï¸ Step 2: Visualize Generated Data\n",
    "Let's preview the generated frames and masks to ensure they're correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "sample_frame = cv2.imread(f\"{CONFIG['input_dir']}/frame_000.png\")\n",
    "sample_mask = cv2.imread(f\"{CONFIG['mask_dir']}/mask_000.png\", 0)\n",
    "\n",
    "if sample_frame is not None and sample_mask is not None:\n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    sample_frame_rgb = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original frame\n",
    "    axes[0].imshow(sample_frame_rgb)\n",
    "    axes[0].set_title('ğŸ¬ Original Frame\\n(with subtitle)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Mask\n",
    "    axes[1].imshow(sample_mask, cmap='gray')\n",
    "    axes[1].set_title('ğŸ­ Subtitle Mask\\n(white = remove)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = sample_frame_rgb.copy()\n",
    "    overlay[sample_mask > 0] = [255, 0, 0]  # Red overlay on subtitle area\n",
    "    combined = cv2.addWeighted(sample_frame_rgb, 0.7, overlay, 0.3, 0)\n",
    "    axes[2].imshow(combined)\n",
    "    axes[2].set_title('ğŸ” Frame + Mask Overlay\\n(red = area to inpaint)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Sample data visualization complete\")\n",
    "else:\n",
    "    print(\"âŒ Could not load sample data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29716a6f",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 3: Pipeline Instructions\n",
    "Based on your environment, here are the next steps to complete the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ Pipeline Instructions:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if env_config['in_docker']:\n",
    "    print(\"ğŸ³ Running in Docker - Pipeline will execute automatically!\")\n",
    "    print(\"   The entrypoint script will handle LaMa and ZITS++ execution.\")\n",
    "elif env_config['in_colab']:\n",
    "    print(\"â˜ï¸  Running in Google Colab:\")\n",
    "    print(\"   1. Install dependencies: !pip install boto3 lama-cleaner\")\n",
    "    print(\"   2. Clone repositories manually\")\n",
    "    print(\"   3. Run commands below\")\n",
    "elif env_config['aws_configured']:\n",
    "    print(\"â˜ï¸  Running with AWS support:\")\n",
    "    print(\"   \udc33 Recommended: Use AWS Docker mode\")\n",
    "    print(\"   cd .. && ./test_pipeline.sh aws gpu s3://bucket/video.mp4 s3://bucket/output/\")\n",
    "    print()\n",
    "    print(\"ğŸ”§ Alternative: Manual AWS setup\")\n",
    "    print(\"   Follow the commands below with S3 integration\")\n",
    "else:\n",
    "    print(\"\ud83dğŸ’» Running locally - Choose your preferred method:\")\n",
    "    print()\n",
    "    print(\"ğŸ³ Option 1: Docker (Recommended)\")\n",
    "    print(\"   cd .. && ./test_pipeline.sh docker\")\n",
    "    print()\n",
    "    print(\"â˜ï¸  Option 2: AWS with S3\")\n",
    "    print(\"   # Set AWS credentials first\")\n",
    "    print(\"   export AWS_ACCESS_KEY_ID=your_key\")\n",
    "    print(\"   export AWS_SECRET_ACCESS_KEY=your_secret\")\n",
    "    print(\"   cd .. && ./test_pipeline.sh aws gpu s3://bucket/video.mp4 s3://bucket/output/\")\n",
    "    print()\n",
    "    print(\"ğŸ”§ Option 3: Manual Setup\")\n",
    "    print(\"   Follow the commands below:\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ“‹ Manual Commands:\")\n",
    "print()\n",
    "print(\"ğŸ¨ LaMa Inpainting:\")\n",
    "print(\"   git clone https://github.com/advimman/lama.git\")\n",
    "print(\"   cd lama && pip install -r requirements.txt\")\n",
    "weight_path = \"big-lama\" if env_config['gpu_available'] else \"PATH_TO_LAMA_WEIGHTS\"\n",
    "print(f\"   python bin/predict.py model.path={weight_path} \\\\\")\n",
    "print(f\"       indir={CONFIG['input_dir']} \\\\\")\n",
    "print(f\"       maskdir={CONFIG['mask_dir']} \\\\\")\n",
    "print(f\"       outdir={CONFIG['lama_output_dir']}\")\n",
    "print()\n",
    "print(\"ğŸ¬ ZITS++ Temporal Refinement:\")\n",
    "print(\"   git clone https://github.com/DQiaole/ZITS_plus_plus.git\")\n",
    "print(\"   cd ZITS_plus_plus && pip install -r requirements.txt\")\n",
    "print(f\"   python test_video.py \\\\\")\n",
    "print(f\"       --video_root {CONFIG['input_dir']} \\\\\")\n",
    "print(f\"       --mask_root {CONFIG['mask_dir']} \\\\\")\n",
    "print(f\"       --inpainted_root {CONFIG['lama_output_dir']} \\\\\")\n",
    "print(f\"       --output_root {CONFIG['zits_output_dir']}\")\n",
    "print()\n",
    "print(\"ğŸï¸  Final Video Assembly:\")\n",
    "print(f\"   ffmpeg -framerate 30 -i {CONFIG['zits_output_dir']}/frame_%05d.png \\\\\")\n",
    "print(\"       -c:v libx264 -pix_fmt yuv420p output_clean.mp4\")\n",
    "print()\n",
    "\n",
    "print(\"âœ¨ AWS/S3 Integration:\")\n",
    "if env_config['aws_configured']:\n",
    "    print(\"   âœ… AWS configured - S3 operations available\")\n",
    "    if CONFIG['s3_input_video']:\n",
    "        print(f\"   ğŸ“¥ S3 Input: {CONFIG['s3_input_video']}\")\n",
    "    if CONFIG['s3_output_location']:\n",
    "        print(f\"   ğŸ“¤ S3 Output: {CONFIG['s3_output_location']}\")\n",
    "else:\n",
    "    print(\"   ğŸ”§ To enable S3 support:\")\n",
    "    print(\"     pip install boto3\")\n",
    "    print(\"     export AWS_ACCESS_KEY_ID=your_key\")\n",
    "    print(\"     export AWS_SECRET_ACCESS_KEY=your_secret\")\n",
    "\n",
    "print()\n",
    "print(\"âœ¨ Deployment Options:\")\n",
    "if env_config['gpu_available']:\n",
    "    print(\"   ğŸš€ GPU detected - processing will be much faster!\")\n",
    "    print(\"   ğŸ“¥ LaMa weights will be auto-downloaded in Docker mode\")\n",
    "    print(\"   ğŸ’¡ Use GPU instances (p3, p4, g4) on AWS for best performance\")\n",
    "else:\n",
    "    print(\"   ğŸ’» CPU mode - processing will take longer but still work\")\n",
    "    print(\"   ğŸ“¥ Download LaMa weights manually for local setup\")\n",
    "    print(\"   ğŸ’¡ Use compute-optimized instances (c5, c6i) on AWS\")\n",
    "\n",
    "print(f\"   ğŸ“ All output will be in: {CONFIG['zits_output_dir']}\")\n",
    "print(\"   ğŸ”„ Rerun this notebook to generate new test data\")\n",
    "print(\"   â˜ï¸  Use AWS for production workloads with large videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbab2d4",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 4: Status Check\n",
    "Monitor the pipeline progress and check outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pipeline_status():\n",
    "    \"\"\"Check the status of pipeline outputs.\"\"\"\n",
    "    status = {\n",
    "        'input_frames': len([f for f in os.listdir(CONFIG['input_dir']) if f.endswith('.png')]) if os.path.exists(CONFIG['input_dir']) else 0,\n",
    "        'masks': len([f for f in os.listdir(CONFIG['mask_dir']) if f.endswith('.png')]) if os.path.exists(CONFIG['mask_dir']) else 0,\n",
    "        'lama_output': len([f for f in os.listdir(CONFIG['lama_output_dir']) if f.endswith('.png')]) if os.path.exists(CONFIG['lama_output_dir']) else 0,\n",
    "        'zits_output': len([f for f in os.listdir(CONFIG['zits_output_dir']) if f.endswith('.png')]) if os.path.exists(CONFIG['zits_output_dir']) else 0,\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“Š Pipeline Status:\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"ğŸ“ Input frames: {status['input_frames']} {'âœ…' if status['input_frames'] > 0 else 'âŒ'}\")\n",
    "    print(f\"ğŸ­ Masks: {status['masks']} {'âœ…' if status['masks'] > 0 else 'âŒ'}\")\n",
    "    print(f\"ğŸ¨ LaMa output: {status['lama_output']} {'âœ…' if status['lama_output'] > 0 else 'â³'}\")\n",
    "    print(f\"ğŸ¬ ZITS output: {status['zits_output']} {'âœ…' if status['zits_output'] > 0 else 'â³'}\")\n",
    "    \n",
    "    if status['zits_output'] > 0:\n",
    "        print(\"\\nğŸ‰ Pipeline completed successfully!\")\n",
    "        print(f\"   Final results: {CONFIG['zits_output_dir']}\")\n",
    "        return True\n",
    "    elif status['lama_output'] > 0:\n",
    "        print(\"\\nâ³ LaMa completed, ZITS++ pending...\")\n",
    "        return False\n",
    "    elif status['input_frames'] > 0:\n",
    "        print(\"\\nâ³ Test data ready, processing pending...\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\nâŒ No data found - run the generation cells above\")\n",
    "        return False\n",
    "\n",
    "# Check current status\n",
    "pipeline_complete = check_pipeline_status()\n",
    "\n",
    "if pipeline_complete:\n",
    "    print(\"\\nğŸ¯ Next steps:\")\n",
    "    print(\"   1. Review output frames\")\n",
    "    print(\"   2. Create final video with FFmpeg\")\n",
    "    print(\"   3. Process your own video data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
